{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.merge import concatenate, add, multiply\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model and preprocessing for the model is adopted from https://github.com/aerdem4/kaggle-quora-dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_occurence = 100\n",
    "unknown = \"memento\"\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "BATCH_SIZE = 1025\n",
    "n_folds=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = set(stopwords.words('english'))\n",
    "\n",
    "print(\"Load glove embedding...\")\n",
    "glove_file = \"./data/word2vec.glove.840B.300d.txt\"\n",
    "glove_model = KeyedVectors.load_word2vec_format(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Load data...\")\n",
    "train = pd.read_csv(\"./train.csv\", names=['row_ID', 'text_a_ID', 'text_b_ID', 'text_a_text', 'text_b_text', 'have_same_meaning'], index_col=0)\n",
    "test = pd.read_csv(\"./test.csv\", names=['row_ID', 'text_a_ID', 'text_b_ID', 'text_a_text', 'text_b_text', 'have_same_meaning'], index_col=0)\n",
    "submission_sample = pd.read_csv(\"./sample_submission_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load train features...\n"
     ]
    }
   ],
   "source": [
    "print(\"Load train features...\")\n",
    "train_nlp_features = pd.read_csv(\"data/nlp_features_train.csv\")\n",
    "train_non_nlp_features = pd.read_csv(\"data/non_nlp_features_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load test features...\n"
     ]
    }
   ],
   "source": [
    "print(\"Load test features...\")\n",
    "test_nlp_features = pd.read_csv(\"data/nlp_features_train.csv\")\n",
    "test_non_nlp_features = pd.read_csv(\"data/non_nlp_features_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word, lemmatizer):\n",
    "    if len(word) < 4:\n",
    "        return word\n",
    "    return lemmatizer.lemmatize(lemmatizer.lemmatize(word, \"n\"), \"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(q):\n",
    "    # Adopted from https://github.com/aerdem4/kaggle-quora-dup\n",
    "    q = q.lower().replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\") \\\n",
    "        .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\") \\\n",
    "        .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\") \\\n",
    "        .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\") \\\n",
    "        .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\") \\\n",
    "        .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \") \\\n",
    "        .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\"=\", \" equal \").replace(\"+\", \" plus \")\n",
    "    q = re.sub('[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]', ' ', q)\n",
    "    q = re.sub(r\"([0-9]+)000000\", r\"\\1m\", q)\n",
    "    q = re.sub(r\"([0-9]+)000\", r\"\\1k\", q)\n",
    "    q = ' '.join([lemmatize(w, lemmatizer) for w in q.split()])\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(embedding_matrix, nb_words, n_features):    \n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "    lstm_layer = LSTM(75, recurrent_dropout=0.2)\n",
    "\n",
    "    sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "    x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "    sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "    y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "    features_input = Input(shape=(n_features,), dtype=\"float32\")\n",
    "    features_dense = BatchNormalization()(features_input)\n",
    "    features_dense = Dense(200, activation=\"relu\")(features_dense)\n",
    "    features_dense = Dropout(0.2)(features_dense)\n",
    "\n",
    "    addition = add([x1, y1])\n",
    "    minus_y1 = Lambda(lambda x: -x)(y1)\n",
    "    merged = add([x1, minus_y1])\n",
    "    merged = multiply([merged, merged])\n",
    "    merged = concatenate([merged, addition])\n",
    "    merged = Dropout(0.4)(merged)\n",
    "\n",
    "    merged = concatenate([merged, features_dense])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = GaussianNoise(0.1)(merged)\n",
    "\n",
    "    merged = Dense(150, activation=\"relu\")(merged)\n",
    "    merged = Dropout(0.2)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "\n",
    "    out = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = Model(inputs=[sequence_1_input, sequence_2_input, features_input], outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(s):\n",
    "    return any(i.isdigit() for i in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(q):\n",
    "    processed_q = []\n",
    "    surplus_q = []\n",
    "    numbers_q = []\n",
    "    new_unknown = True\n",
    "    for word in q.split()[::-1]:\n",
    "        if word in top_words:\n",
    "            processed_q = [word] + processed_q\n",
    "            new_unknown = True\n",
    "        elif word not in en_stop:\n",
    "            if new_unknown:\n",
    "                processed_q = [unknown] + processed_q\n",
    "                new_unknown = False\n",
    "            if is_numeric(word):\n",
    "                numbers_q = [word] + numbers_q\n",
    "            else:\n",
    "                surplus_q = [word] + surplus_q\n",
    "        else:\n",
    "            new_memento = True\n",
    "        if len(processed_q) == MAX_SEQUENCE_LENGTH:\n",
    "            break\n",
    "    return \" \".join(processed_q), set(surplus_q), set(numbers_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute train set features and embedding...\n"
     ]
    }
   ],
   "source": [
    "print(\"Compute train set features and embedding...\")\n",
    "train[\"text_a_text_clean\"] = train[\"text_a_text\"].fillna(\"\").apply(clean)\n",
    "train[\"text_b_text_clean\"] = train[\"text_b_text\"].fillna(\"\").apply(clean)\n",
    "\n",
    "unique_questions = pd.Series(train[\"text_a_text_clean\"] + train[\"text_b_text_clean\"]).unique()\n",
    "count_vectorizer = CountVectorizer(lowercase=True, token_pattern=\"\\S+\", min_df=min_occurence)\n",
    "count_vectorizer.fit(unique_questions)\n",
    "\n",
    "top_words = set(count_vectorizer.vocabulary_.keys())\n",
    "top_words.add(unknown)\n",
    "\n",
    "train_q_a_features = train[\"text_a_text_clean\"].apply(preprocess)\n",
    "train_q_b_features = train[\"text_b_text_clean\"].apply(preprocess)\n",
    "\n",
    "train_q_a = train_q_a_features.fillna(\"\").apply(lambda x: x[0])\n",
    "train_q_b = train_q_b_features.fillna(\"\").apply(lambda x: x[0])\n",
    "\n",
    "train_intermediate_df = pd.DataFrame(index=train.index)\n",
    "train_intermediate_df[\"surplus_a\"] = train_q_a_features.apply(lambda x: x[1])\n",
    "train_intermediate_df[\"surplus_b\"] = train_q_b_features.apply(lambda x: x[1])\n",
    "train_intermediate_df[\"number_a\"] = train_q_a_features.apply(lambda x: x[2])\n",
    "train_intermediate_df[\"number_b\"] = train_q_b_features.apply(lambda x: x[2])\n",
    "\n",
    "train_features_df = pd.DataFrame(index=train.index)\n",
    "train_features_df[\"surplus_intersection\"] = train_intermediate_df.apply(lambda x: len(x.surplus_a.intersection(x.surplus_b)), axis=1)\n",
    "train_features_df[\"surplus_union\"] = train_intermediate_df.apply(lambda x: len(x.surplus_a.union(x.surplus_b)), axis=1)\n",
    "train_features_df[\"number_intersection\"] = train_intermediate_df.apply(lambda x: len(x.number_a.intersection(x.number_b)), axis=1)\n",
    "train_features_df[\"number_union\"] = train_intermediate_df.apply(lambda x: len(x.number_a.union(x.number_b)), axis=1)\n",
    "\n",
    "features_train = np.hstack((train_nlp_features, train_non_nlp_features, train_features_df))\n",
    "n_features = features_train.shape[1]\n",
    "\n",
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(np.append(train_q_a, train_q_b))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "train_q_a_padded = pad_sequences(tokenizer.texts_to_sequences(train_q_a), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "train_q_b_padded = pad_sequences(tokenizer.texts_to_sequences(train_q_a), maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute test set features and embedding...\n"
     ]
    }
   ],
   "source": [
    "print(\"Compute test set features and embedding...\")\n",
    "test[\"text_a_text_clean\"] = test[\"text_a_text\"].fillna(\"\").apply(clean)\n",
    "test[\"text_b_text_clean\"] = test[\"text_b_text\"].fillna(\"\").apply(clean)\n",
    "\n",
    "q_a_features_test = test[\"text_a_text_clean\"].apply(preprocess)\n",
    "q_b_features_test = test[\"text_b_text_clean\"].apply(preprocess)\n",
    "\n",
    "test_q_a = q_a_features_test.apply(lambda x: x[0])\n",
    "test_q_b = q_b_features_test.apply(lambda x: x[0])\n",
    "\n",
    "test_intermediate_df = pd.DataFrame(index=test.index)\n",
    "test_intermediate_df[\"surplus_a\"] = q_a_features_test.apply(lambda x: x[1])\n",
    "test_intermediate_df[\"surplus_b\"] = q_b_features_test.apply(lambda x: x[1])\n",
    "test_intermediate_df[\"number_a\"] = q_a_features_test.apply(lambda x: x[2])\n",
    "test_intermediate_df[\"number_b\"] = q_b_features_test.apply(lambda x: x[2])\n",
    "\n",
    "test_features_df = pd.DataFrame(index=test.index)\n",
    "test_features_df[\"surplus_intersection\"] = test_intermediate_df.apply(lambda x: len(x.surplus_a.intersection(x.surplus_b)), axis=1)\n",
    "test_features_df[\"surplus_union\"] = test_intermediate_df.apply(lambda x: len(x.surplus_a.union(x.surplus_b)), axis=1)\n",
    "test_features_df[\"number_intersection\"] = test_intermediate_df.apply(lambda x: len(x.number_a.intersection(x.number_b)), axis=1)\n",
    "test_features_df[\"number_union\"] = test_intermediate_df.apply(lambda x: len(x.number_a.union(x.number_b)), axis=1)\n",
    "\n",
    "features_test = np.hstack((test_nlp_features, test_non_nlp_features, train_features_df))\n",
    "\n",
    "test_q_a_padded = pad_sequences(tokenizer.texts_to_sequences(test_q_a), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_q_b_padded = pad_sequences(tokenizer.texts_to_sequences(test_q_a), maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(train[\"have_same_meaning\"])\n",
    "nb_words = len(word_index) + 1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector=None\n",
    "    if word in glove_model.wv:\n",
    "        embedding_vector = glove_model.wv[word]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=n_folds, shuffle=True)\n",
    "model_count = 0\n",
    "\n",
    "train_ensemble = pd.DataFrame(index=train.index)\n",
    "test_ensemble = pd.DataFrame(index=test.index)\n",
    "\n",
    "train_ensemble[\"pred_lstm\"] = 0\n",
    "test_ensemble[\"pred_lstm\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72089 samples, validate on 8011 samples\n",
      "Epoch 1/50\n",
      "72089/72089 [==============================] - 48s 667us/step - loss: 0.2475 - val_loss: 0.1711\n",
      "Epoch 2/50\n",
      "72089/72089 [==============================] - 17s 234us/step - loss: 0.1814 - val_loss: 0.1588\n",
      "Epoch 3/50\n",
      "72089/72089 [==============================] - 13s 177us/step - loss: 0.1680 - val_loss: 0.1632\n",
      "Epoch 4/50\n",
      "72089/72089 [==============================] - 13s 177us/step - loss: 0.1604 - val_loss: 0.1526\n",
      "Epoch 5/50\n",
      "72089/72089 [==============================] - 13s 177us/step - loss: 0.1545 - val_loss: 0.1498\n",
      "Epoch 6/50\n",
      "72089/72089 [==============================] - 13s 177us/step - loss: 0.1507 - val_loss: 0.1492\n",
      "Epoch 7/50\n",
      "72089/72089 [==============================] - 13s 185us/step - loss: 0.1460 - val_loss: 0.1498\n",
      "Epoch 8/50\n",
      "72089/72089 [==============================] - 13s 185us/step - loss: 0.1430 - val_loss: 0.1450\n",
      "Epoch 9/50\n",
      "72089/72089 [==============================] - 13s 182us/step - loss: 0.1403 - val_loss: 0.1420\n",
      "Epoch 10/50\n",
      "72089/72089 [==============================] - 13s 184us/step - loss: 0.1351 - val_loss: 0.1408\n",
      "Epoch 11/50\n",
      "72089/72089 [==============================] - 13s 179us/step - loss: 0.1323 - val_loss: 0.1423\n",
      "Epoch 12/50\n",
      "72089/72089 [==============================] - 14s 188us/step - loss: 0.1291 - val_loss: 0.1437\n",
      "Epoch 13/50\n",
      "72089/72089 [==============================] - 14s 189us/step - loss: 0.1249 - val_loss: 0.1416\n",
      "Epoch 14/50\n",
      "72089/72089 [==============================] - 13s 185us/step - loss: 0.1219 - val_loss: 0.1386\n",
      "Epoch 15/50\n",
      "72089/72089 [==============================] - 13s 182us/step - loss: 0.1186 - val_loss: 0.1462\n",
      "Epoch 16/50\n",
      "72089/72089 [==============================] - 13s 182us/step - loss: 0.1150 - val_loss: 0.1507\n",
      "Epoch 17/50\n",
      "72089/72089 [==============================] - 13s 181us/step - loss: 0.1120 - val_loss: 0.1419\n",
      "Epoch 18/50\n",
      "72089/72089 [==============================] - 13s 176us/step - loss: 0.1097 - val_loss: 0.1448\n",
      "Epoch 19/50\n",
      "72089/72089 [==============================] - 13s 178us/step - loss: 0.1066 - val_loss: 0.1468\n",
      "0 validation loss: 0.13858853218324352\n",
      "8011/8011 [==============================] - 7s 868us/step\n",
      "20179/20179 [==============================] - 2s 78us/step\n",
      "Train on 72090 samples, validate on 8010 samples\n",
      "Epoch 1/50\n",
      "72090/72090 [==============================] - 33s 455us/step - loss: 0.2581 - val_loss: 0.1799\n",
      "Epoch 2/50\n",
      "72090/72090 [==============================] - 13s 179us/step - loss: 0.1834 - val_loss: 0.1674\n",
      "Epoch 3/50\n",
      "72090/72090 [==============================] - 13s 179us/step - loss: 0.1681 - val_loss: 0.1744\n",
      "Epoch 4/50\n",
      "72090/72090 [==============================] - 13s 182us/step - loss: 0.1592 - val_loss: 0.1587\n",
      "Epoch 5/50\n",
      "72090/72090 [==============================] - 13s 179us/step - loss: 0.1538 - val_loss: 0.1544\n",
      "Epoch 6/50\n",
      "72090/72090 [==============================] - 13s 181us/step - loss: 0.1498 - val_loss: 0.1554\n",
      "Epoch 7/50\n",
      "72090/72090 [==============================] - 13s 181us/step - loss: 0.1451 - val_loss: 0.1582\n",
      "Epoch 8/50\n",
      "72090/72090 [==============================] - 13s 180us/step - loss: 0.1412 - val_loss: 0.1521\n",
      "Epoch 9/50\n",
      "72090/72090 [==============================] - 13s 178us/step - loss: 0.1386 - val_loss: 0.1501\n",
      "Epoch 10/50\n",
      "72090/72090 [==============================] - 13s 179us/step - loss: 0.1337 - val_loss: 0.1492\n",
      "Epoch 11/50\n",
      "72090/72090 [==============================] - 13s 181us/step - loss: 0.1294 - val_loss: 0.1597\n",
      "Epoch 12/50\n",
      "72090/72090 [==============================] - 13s 177us/step - loss: 0.1271 - val_loss: 0.1513\n",
      "Epoch 13/50\n",
      "72090/72090 [==============================] - 13s 179us/step - loss: 0.1216 - val_loss: 0.1515\n",
      "Epoch 14/50\n",
      "72090/72090 [==============================] - 13s 181us/step - loss: 0.1195 - val_loss: 0.1577\n",
      "Epoch 15/50\n",
      "72090/72090 [==============================] - 13s 180us/step - loss: 0.1160 - val_loss: 0.1532\n",
      "1 validation loss: 0.14920965377032086\n",
      "8010/8010 [==============================] - 7s 918us/step\n",
      "20179/20179 [==============================] - 2s 76us/step\n",
      "Train on 72090 samples, validate on 8010 samples\n",
      "Epoch 1/50\n",
      "72090/72090 [==============================] - 33s 453us/step - loss: 0.2545 - val_loss: 0.1743\n",
      "Epoch 2/50\n",
      "72090/72090 [==============================] - 13s 177us/step - loss: 0.1826 - val_loss: 0.1679\n",
      "Epoch 3/50\n",
      "72090/72090 [==============================] - 13s 178us/step - loss: 0.1688 - val_loss: 0.1557\n",
      "Epoch 4/50\n",
      "72090/72090 [==============================] - 13s 177us/step - loss: 0.1610 - val_loss: 0.1518\n",
      "Epoch 5/50\n",
      "72090/72090 [==============================] - 13s 183us/step - loss: 0.1549 - val_loss: 0.1575\n",
      "Epoch 6/50\n",
      "72090/72090 [==============================] - 13s 181us/step - loss: 0.1496 - val_loss: 0.1504\n",
      "Epoch 7/50\n",
      "72090/72090 [==============================] - 13s 186us/step - loss: 0.1467 - val_loss: 0.1483\n",
      "Epoch 8/50\n",
      "72090/72090 [==============================] - 13s 184us/step - loss: 0.1424 - val_loss: 0.1486\n",
      "Epoch 9/50\n",
      "72090/72090 [==============================] - 13s 184us/step - loss: 0.1379 - val_loss: 0.1452\n",
      "Epoch 10/50\n",
      "72090/72090 [==============================] - 13s 185us/step - loss: 0.1347 - val_loss: 0.1449\n",
      "Epoch 11/50\n",
      "72090/72090 [==============================] - 13s 180us/step - loss: 0.1300 - val_loss: 0.1447\n",
      "Epoch 12/50\n",
      "72090/72090 [==============================] - 13s 180us/step - loss: 0.1266 - val_loss: 0.1518\n",
      "Epoch 13/50\n",
      "72090/72090 [==============================] - 13s 179us/step - loss: 0.1239 - val_loss: 0.1492\n",
      "Epoch 14/50\n",
      "72090/72090 [==============================] - 13s 179us/step - loss: 0.1207 - val_loss: 0.1492\n",
      "Epoch 15/50\n",
      "72090/72090 [==============================] - 13s 180us/step - loss: 0.1158 - val_loss: 0.1680\n",
      "Epoch 16/50\n",
      "72090/72090 [==============================] - 13s 182us/step - loss: 0.1120 - val_loss: 0.1493\n",
      "2 validation loss: 0.1446659310201134\n",
      "8010/8010 [==============================] - 8s 989us/step\n",
      "20179/20179 [==============================] - 2s 77us/step\n",
      "Train on 72090 samples, validate on 8010 samples\n",
      "Epoch 1/50\n",
      "72090/72090 [==============================] - 35s 492us/step - loss: 0.2525 - val_loss: 0.1868\n",
      "Epoch 2/50\n",
      "72090/72090 [==============================] - 13s 178us/step - loss: 0.1827 - val_loss: 0.1759\n",
      "Epoch 3/50\n",
      "72090/72090 [==============================] - 13s 179us/step - loss: 0.1690 - val_loss: 0.1807\n",
      "Epoch 4/50\n",
      "72090/72090 [==============================] - 14s 188us/step - loss: 0.1600 - val_loss: 0.1630\n",
      "Epoch 5/50\n",
      "72090/72090 [==============================] - 13s 179us/step - loss: 0.1541 - val_loss: 0.1614\n",
      "Epoch 6/50\n",
      "72090/72090 [==============================] - 13s 187us/step - loss: 0.1502 - val_loss: 0.1589\n",
      "Epoch 7/50\n",
      "72090/72090 [==============================] - 13s 178us/step - loss: 0.1448 - val_loss: 0.1534\n",
      "Epoch 8/50\n",
      "72090/72090 [==============================] - 14s 189us/step - loss: 0.1425 - val_loss: 0.1557\n",
      "Epoch 9/50\n",
      "72090/72090 [==============================] - 13s 179us/step - loss: 0.1375 - val_loss: 0.1548\n",
      "Epoch 10/50\n",
      "72090/72090 [==============================] - 13s 177us/step - loss: 0.1336 - val_loss: 0.1565\n",
      "Epoch 11/50\n",
      "72090/72090 [==============================] - 14s 188us/step - loss: 0.1293 - val_loss: 0.1575\n",
      "Epoch 12/50\n",
      "72090/72090 [==============================] - 13s 182us/step - loss: 0.1260 - val_loss: 0.1583\n",
      "3 validation loss: 0.15344385257672133\n",
      "8010/8010 [==============================] - 8s 1ms/step\n",
      "20179/20179 [==============================] - 2s 75us/step\n",
      "Train on 72090 samples, validate on 8010 samples\n",
      "Epoch 1/50\n",
      "72090/72090 [==============================] - 35s 489us/step - loss: 0.2482 - val_loss: 0.1779\n",
      "Epoch 2/50\n",
      "72090/72090 [==============================] - 13s 180us/step - loss: 0.1818 - val_loss: 0.1669\n",
      "Epoch 3/50\n",
      "72090/72090 [==============================] - 13s 177us/step - loss: 0.1669 - val_loss: 0.1611\n",
      "Epoch 4/50\n",
      "72090/72090 [==============================] - 13s 177us/step - loss: 0.1603 - val_loss: 0.1599\n",
      "Epoch 5/50\n",
      "72090/72090 [==============================] - 13s 180us/step - loss: 0.1541 - val_loss: 0.1600\n",
      "Epoch 6/50\n",
      "72090/72090 [==============================] - 14s 192us/step - loss: 0.1503 - val_loss: 0.1543\n",
      "Epoch 7/50\n",
      "72090/72090 [==============================] - 13s 180us/step - loss: 0.1442 - val_loss: 0.1535\n",
      "Epoch 8/50\n",
      "72090/72090 [==============================] - 13s 178us/step - loss: 0.1403 - val_loss: 0.1569\n",
      "Epoch 9/50\n",
      "72090/72090 [==============================] - 13s 178us/step - loss: 0.1366 - val_loss: 0.1539\n",
      "Epoch 10/50\n",
      "72090/72090 [==============================] - 13s 185us/step - loss: 0.1329 - val_loss: 0.1540\n",
      "Epoch 11/50\n",
      "72090/72090 [==============================] - 15s 211us/step - loss: 0.1296 - val_loss: 0.1590\n",
      "Epoch 12/50\n",
      "72090/72090 [==============================] - 14s 189us/step - loss: 0.1247 - val_loss: 0.1558\n",
      "4 validation loss: 0.15346241791670986\n",
      "8010/8010 [==============================] - 9s 1ms/step\n",
      "20179/20179 [==============================] - 2s 77us/step\n",
      "Train on 72090 samples, validate on 8010 samples\n",
      "Epoch 1/50\n",
      "72090/72090 [==============================] - 39s 538us/step - loss: 0.2509 - val_loss: 0.1743\n",
      "Epoch 2/50\n",
      "72090/72090 [==============================] - 13s 179us/step - loss: 0.1815 - val_loss: 0.1635\n",
      "Epoch 3/50\n",
      "72090/72090 [==============================] - 13s 179us/step - loss: 0.1664 - val_loss: 0.1624\n",
      "Epoch 4/50\n",
      "72090/72090 [==============================] - 13s 181us/step - loss: 0.1597 - val_loss: 0.1567\n",
      "Epoch 5/50\n",
      "72090/72090 [==============================] - 14s 191us/step - loss: 0.1561 - val_loss: 0.1596\n",
      "Epoch 6/50\n",
      "72090/72090 [==============================] - 13s 184us/step - loss: 0.1512 - val_loss: 0.1537\n",
      "Epoch 7/50\n",
      "72090/72090 [==============================] - 13s 186us/step - loss: 0.1468 - val_loss: 0.1508\n",
      "Epoch 8/50\n",
      "72090/72090 [==============================] - 13s 179us/step - loss: 0.1419 - val_loss: 0.1576\n",
      "Epoch 9/50\n",
      "72090/72090 [==============================] - 13s 180us/step - loss: 0.1372 - val_loss: 0.1518\n",
      "Epoch 10/50\n",
      "72090/72090 [==============================] - 13s 177us/step - loss: 0.1350 - val_loss: 0.1496\n",
      "Epoch 11/50\n",
      "72090/72090 [==============================] - 13s 178us/step - loss: 0.1301 - val_loss: 0.1611\n",
      "Epoch 12/50\n",
      "72090/72090 [==============================] - 13s 178us/step - loss: 0.1262 - val_loss: 0.1481\n",
      "Epoch 13/50\n",
      "72090/72090 [==============================] - 13s 181us/step - loss: 0.1236 - val_loss: 0.1504\n",
      "Epoch 14/50\n",
      "72090/72090 [==============================] - 13s 179us/step - loss: 0.1206 - val_loss: 0.1676\n",
      "Epoch 15/50\n",
      "72090/72090 [==============================] - 13s 182us/step - loss: 0.1155 - val_loss: 0.1535\n",
      "Epoch 16/50\n",
      "72090/72090 [==============================] - 13s 178us/step - loss: 0.1121 - val_loss: 0.1519\n",
      "Epoch 17/50\n",
      "72090/72090 [==============================] - 13s 180us/step - loss: 0.1099 - val_loss: 0.1531\n",
      "5 validation loss: 0.148126199292705\n",
      "8010/8010 [==============================] - 9s 1ms/step\n",
      "20179/20179 [==============================] - 2s 75us/step\n",
      "Train on 72090 samples, validate on 8010 samples\n",
      "Epoch 1/50\n",
      "72090/72090 [==============================] - 39s 536us/step - loss: 0.2567 - val_loss: 0.1764\n",
      "Epoch 2/50\n",
      "72090/72090 [==============================] - 13s 179us/step - loss: 0.1824 - val_loss: 0.1701\n",
      "Epoch 3/50\n",
      "72090/72090 [==============================] - 13s 178us/step - loss: 0.1698 - val_loss: 0.1639\n",
      "Epoch 4/50\n",
      "72090/72090 [==============================] - 13s 180us/step - loss: 0.1616 - val_loss: 0.1560\n",
      "Epoch 5/50\n",
      "72090/72090 [==============================] - 13s 177us/step - loss: 0.1569 - val_loss: 0.1655\n",
      "Epoch 6/50\n",
      "72090/72090 [==============================] - 13s 186us/step - loss: 0.1516 - val_loss: 0.1540\n",
      "Epoch 7/50\n",
      "72090/72090 [==============================] - 13s 178us/step - loss: 0.1477 - val_loss: 0.1556\n",
      "Epoch 8/50\n",
      "72090/72090 [==============================] - 14s 188us/step - loss: 0.1424 - val_loss: 0.1469\n",
      "Epoch 9/50\n",
      "72090/72090 [==============================] - 13s 177us/step - loss: 0.1398 - val_loss: 0.1538\n",
      "Epoch 10/50\n",
      "72090/72090 [==============================] - 13s 177us/step - loss: 0.1353 - val_loss: 0.1459\n",
      "Epoch 11/50\n",
      "72090/72090 [==============================] - 13s 186us/step - loss: 0.1319 - val_loss: 0.1469\n",
      "Epoch 12/50\n",
      "72090/72090 [==============================] - 13s 176us/step - loss: 0.1289 - val_loss: 0.1545\n",
      "Epoch 13/50\n",
      "26650/72090 [==========>...................] - ETA: 7s - loss: 0.1214"
     ]
    }
   ],
   "source": [
    "for train_indices, validation_indices in kfold.split(train[\"have_same_meaning\"], train[\"have_same_meaning\"]):\n",
    "    train_fold_a = train_q_a_padded[train_indices]\n",
    "    train_fold_b = train_q_b_padded[train_indices]\n",
    "    train_fold_features = features_train[train_indices]\n",
    "    train_fold_labels = labels[train_indices]\n",
    "\n",
    "    val_fold_a = train_q_a_padded[validation_indices]\n",
    "    val_fold_b = train_q_b_padded[validation_indices]\n",
    "    val_fold_features = features_train[validation_indices]\n",
    "    val_fold_labels = labels[validation_indices]\n",
    "    \n",
    "    model = get_model(embedding_matrix, nb_words, n_features)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\")\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    best_model_path = \"best_model\" + str(model_count) + \".h5\"\n",
    "    model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    hist = model.fit([train_fold_a, train_fold_b, train_fold_features], train_fold_labels,\n",
    "                     validation_data=([val_fold_a, val_fold_b, val_fold_features], val_fold_labels),\n",
    "                     epochs=50, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                     callbacks=[early_stopping, model_checkpoint], verbose=1)\n",
    "\n",
    "    model.load_weights(best_model_path)\n",
    "    print(model_count, \"validation loss:\", min(hist.history[\"val_loss\"]))\n",
    "    \n",
    "    train_pred = model.predict([val_fold_a, val_fold_b, val_fold_features], batch_size=BATCH_SIZE, verbose=1)\n",
    "    test_pred = model.predict([test_q_a_padded, test_q_b_padded, features_test], batch_size=BATCH_SIZE, verbose=1)\n",
    "    \n",
    "    test_ensemble[\"pred_lstm\"] = test_pred\n",
    "    train_ensemble.loc[validation_indices,\"pred_lstm\"] = train_pred.ravel()\n",
    "\n",
    "    submission = pd.DataFrame({\"Id\": test.index, \"Score\": test_pred.ravel()})\n",
    "    submission.to_csv(\"predictions/preds\" + str(model_count) + \".csv\", index=False)\n",
    "\n",
    "    model_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ensemble.to_csv(\"./predictions/test_ensemble_lstm.csv\", index=False)\n",
    "train_ensemble.to_csv(\"./predictions/train_ensemble_lstm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
