{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.merge import concatenate, add, multiply\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_occurence = 100\n",
    "unknown = \"memento\"\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "BATCH_SIZE = 1024\n",
    "n_folds=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = set(stopwords.words('english'))\n",
    "\n",
    "glove_file = \"./data/word2vec.glove.840B.300d.txt\"\n",
    "glove_model = KeyedVectors.load_word2vec_format(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train.csv\", names=['row_ID', 'text_a_ID', 'text_b_ID', 'text_a_text', 'text_b_text', 'have_same_meaning'], index_col=0)\n",
    "test = pd.read_csv(\"./test.csv\", names=['row_ID', 'text_a_ID', 'text_b_ID', 'text_a_text', 'text_b_text', 'have_same_meaning'], index_col=0)\n",
    "submission_sample = pd.read_csv(\"./sample_submission_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word, lemmatizer):\n",
    "    if len(word) < 4:\n",
    "        return word\n",
    "    return lemmatizer.lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(q):\n",
    "    # Adopted from https://github.com/aerdem4/kaggle-quora-dup\n",
    "    q = str(q).lower()\n",
    "    q = q.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
    "                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
    "                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
    "                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
    "                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
    "                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
    "                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n",
    "    q = re.sub('[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]', ' ', q)\n",
    "    q = re.sub(r\"([0-9]+)000000\", r\"\\1m\", q)\n",
    "    q = re.sub(r\"([0-9]+)000\", r\"\\1k\", q)\n",
    "    q = ' '.join([lemmatize(w,lemmatizer) for w in q.split()])\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(embedding_matrix, nb_words, n_features):    \n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "    lstm_layer = LSTM(75, recurrent_dropout=0.2)\n",
    "\n",
    "    sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "    x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "    sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "    y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "    features_input = Input(shape=(n_features,), dtype=\"float32\")\n",
    "    features_dense = BatchNormalization()(features_input)\n",
    "    features_dense = Dense(200, activation=\"relu\")(features_dense)\n",
    "    features_dense = Dropout(0.2)(features_dense)\n",
    "\n",
    "    addition = add([x1, y1])\n",
    "    minus_y1 = Lambda(lambda x: -x)(y1)\n",
    "    merged = add([x1, minus_y1])\n",
    "    merged = multiply([merged, merged])\n",
    "    merged = concatenate([merged, addition])\n",
    "    merged = Dropout(0.4)(merged)\n",
    "\n",
    "    merged = concatenate([merged, features_dense])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = GaussianNoise(0.1)(merged)\n",
    "\n",
    "    merged = Dense(150, activation=\"relu\")(merged)\n",
    "    merged = Dropout(0.2)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "\n",
    "    out = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = Model(inputs=[sequence_1_input, sequence_2_input, features_input], outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(s):\n",
    "    return any(i.isdigit() for i in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(q):\n",
    "    processed_q = []\n",
    "    surplus_q = []\n",
    "    numbers_q = []\n",
    "    new_unknown = True\n",
    "    for word in q.split()[::-1]:\n",
    "        if word in top_words:\n",
    "            processed_q = [word] + processed_q\n",
    "            new_unknown = True\n",
    "        elif word not in en_stop:\n",
    "            if new_unknown:\n",
    "                processed_q = [unknown] + processed_q\n",
    "                new_unknown = False\n",
    "            if is_numeric(word):\n",
    "                numbers_q = [word] + numbers_q\n",
    "            else:\n",
    "                surplus_q = [word] + surplus_q\n",
    "        else:\n",
    "            new_memento = True\n",
    "        if len(processed_q) == MAX_SEQUENCE_LENGTH:\n",
    "            break\n",
    "    return \" \".join(processed_q), set(surplus_q), set(numbers_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text_a_text_clean\"] = train[\"text_a_text\"].fillna(\"\").apply(clean)\n",
    "train[\"text_b_text_clean\"] = train[\"text_b_text\"].fillna(\"\").apply(clean)\n",
    "\n",
    "unique_questions = pd.Series(train[\"text_a_text_clean\"] + train[\"text_b_text_clean\"]).unique()\n",
    "count_vectorizer = CountVectorizer(lowercase=True, token_pattern=\"\\S+\", min_df=min_occurence)\n",
    "count_vectorizer.fit(unique_questions)\n",
    "\n",
    "top_words = set(count_vectorizer.vocabulary_.keys())\n",
    "top_words.add(unknown)\n",
    "\n",
    "q_a_features = train[\"text_a_text_clean\"].apply(preprocess)\n",
    "q_b_features = train[\"text_b_text_clean\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q_a = q_a_features.apply(lambda x: x[0])\n",
    "train_q_b = q_b_features.apply(lambda x: x[0])\n",
    "\n",
    "train_intermediate_df = pd.DataFrame(index=train.index)\n",
    "\n",
    "train_intermediate_df[\"surplus_a\"] = q_a_features.apply(lambda x: x[1])\n",
    "train_intermediate_df[\"surplus_b\"] = q_b_features.apply(lambda x: x[1])\n",
    "\n",
    "train_intermediate_df[\"number_a\"] = q_a_features.apply(lambda x: x[2])\n",
    "train_intermediate_df[\"number_b\"] = q_b_features.apply(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_df = pd.DataFrame(index=train.index)\n",
    "train_features_df[\"surplus_intersection\"] = train_intermediate_df.apply(lambda x: len(x.surplus_a.intersection(x.surplus_b)), axis=1)\n",
    "train_features_df[\"surplus_union\"] = train_intermediate_df.apply(lambda x: len(x.surplus_a.union(x.surplus_b)), axis=1)\n",
    "train_features_df[\"number_intersection\"] = train_intermediate_df.apply(lambda x: len(x.number_a.intersection(x.number_b)), axis=1)\n",
    "train_features_df[\"number_union\"] = train_intermediate_df.apply(lambda x: len(x.number_a.union(x.number_b)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(np.append(train_q_a, train_q_b))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "train_q_a_padded = pad_sequences(tokenizer.texts_to_sequences(train_q_a), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "train_q_b_padded = pad_sequences(tokenizer.texts_to_sequences(train_q_a), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = np.array(train[\"have_same_meaning\"])\n",
    "\n",
    "nb_words = len(word_index) + 1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector=None\n",
    "    if word in glove_model.wv:\n",
    "        embedding_vector = glove_model.wv[word]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "train_nlp_features = pd.read_csv(\"data/nlp_features_train.csv\")\n",
    "train_non_nlp_features = pd.read_csv(\"data/non_nlp_features_train.csv\")\n",
    "features_train = np.hstack((train_nlp_features, train_non_nlp_features, train_features_df))\n",
    "\n",
    "n_features = features_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test[\"text_a_text_clean\"] = test[\"text_a_text\"].fillna(\"\").apply(clean)\n",
    "# test[\"text_b_text_clean\"] = test[\"text_b_text\"].fillna(\"\").apply(clean)\n",
    "\n",
    "# test_q_a = test[\"text_a_text_clean\"].apply(preprocess)\n",
    "# test_q_b = test[\"text_b_text_clean\"].apply(preprocess)\n",
    "\n",
    "# test_q_a_padded = pad_sequences(tokenizer.texts_to_sequences(test_q_a), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# test_q_a_padded = pad_sequences(tokenizer.texts_to_sequences(test_q_a), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# test_nlp_features = pd.read_csv(\"data/nlp_features_test.csv\")\n",
    "# test_non_nlp_features = pd.read_csv(\"data/non_nlp_features_test.csv\")\n",
    "# features_test = np.hstack((test_nlp_features, test_non_nlp_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=n_folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53400 samples, validate on 26700 samples\n",
      "Epoch 1/15\n",
      "53248/53400 [============================>.] - ETA: 0s - loss: 0.3144"
     ]
    }
   ],
   "source": [
    "for train_indices, validation_indices in kfold.split(train[\"have_same_meaning\"], train[\"have_same_meaning\"]):\n",
    "    model_count+=1\n",
    "    train_fold_a = train_q_a_padded[train_indices]\n",
    "    train_fold_b = train_q_b_padded[train_indices]\n",
    "    train_fold_features = features_train[train_indices]\n",
    "    train_fold_labels = labels[train_indices]\n",
    "\n",
    "    val_fold_a = train_q_a_padded[validation_indices]\n",
    "    val_fold_b = train_q_a_padded[validation_indices]\n",
    "    val_fold_features = features_train[validation_indices]\n",
    "    val_fold_labels = labels[validation_indices]\n",
    "    \n",
    "    model = get_model(embedding_matrix, nb_words, n_features)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    best_model_path = \"best_model\" + str(model_count) + \".h5\"\n",
    "    model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    hist = model.fit([train_fold_a, train_fold_b, train_fold_features], train_fold_labels,\n",
    "                     validation_data=([val_fold_a, val_fold_b, val_fold_features], val_fold_labels),\n",
    "                     epochs=15, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                     callbacks=[early_stopping, model_checkpoint], verbose=1)\n",
    "\n",
    "    model.load_weights(best_model_path)\n",
    "    print(model_count, \"validation loss:\", min(hist.history[\"val_loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
