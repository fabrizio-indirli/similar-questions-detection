{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb \n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from scripts.lgbmHelper import LGBMHelper\n",
    "from scripts.xgbHelper import XGBHelper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = pd.read_csv(\"./train.csv\", names=['row_ID', 'text_a_ID', 'text_b_ID', 'text_a_text', 'text_b_text', 'have_same_meaning'], index_col=0)\n",
    "train_nlp_features = pd.read_csv(\"data/nlp_features_train.csv\")\n",
    "train_non_nlp_features = pd.read_csv(\"data/non_nlp_features_train.csv\")\n",
    "train_distance_features = pd.read_csv(\"data/distance_features_train.csv\")\n",
    "#train = pd.concat((train_nlp_features, train_non_nlp_features, train_distance_features), axis=1)\n",
    "train = pd.concat((train_non_nlp_features, train_distance_features), axis=1)\n",
    "\n",
    "test_all = pd.read_csv(\"./test.csv\", names=['row_ID', 'text_a_ID', 'text_b_ID', 'text_a_text', 'text_b_text', 'have_same_meaning'], index_col=0)\n",
    "test_nlp_features = pd.read_csv(\"data/nlp_features_test.csv\")\n",
    "test_non_nlp_features = pd.read_csv(\"data/non_nlp_features_test.csv\")\n",
    "test_distance_features = pd.read_csv(\"data/distance_features_test.csv\")\n",
    "#test = pd.concat((test_nlp_features, test_non_nlp_features, test_distance_features), axis=1)\n",
    "test = pd.concat((test_non_nlp_features, test_distance_features), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna(axis=1)\n",
    "train =train.replace([np.inf, -np.inf], -1)\n",
    "\n",
    "test = test.dropna(axis=1)\n",
    "test= test.replace([np.inf, -np.inf], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ensemble = train.copy()\n",
    "test_ensemble = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "kfold_splits = list(kfold.split(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [(LGBMHelper(), False),\n",
    "          (LogisticRegression(solver='lbfgs', max_iter=1000), True), \n",
    "          (RandomForestClassifier(), False),\n",
    "          (GradientBoostingClassifier(), False),\n",
    "          (XGBHelper(), False),\n",
    "          (KNeighborsClassifier(n_neighbors=3), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for i, (model,scale) in enumerate(models):\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    print(\"Train model {}\".format(model.__class__.__name__))\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    log_loss_fold = []\n",
    "    print(\"Split \", end=\"\")\n",
    "    for j, (train_indices, test_indices) in enumerate(kfold_splits):\n",
    "        print(str(j+1) + \",\", end=\" \")\n",
    "        X_train = train.iloc[train_indices].astype(np.float64)\n",
    "        y_train = train_all.have_same_meaning.iloc[train_indices].astype(np.float64)\n",
    "        \n",
    "        X_test = train.iloc[test_indices].astype(np.float64)\n",
    "        y_test = train_all.have_same_meaning.iloc[test_indices].astype(np.float64)\n",
    "        \n",
    "        if scale: \n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        train_pred = model.predict_proba(train.iloc[test_indices,:])\n",
    "        test_pred = model.predict_proba(test)\n",
    "\n",
    "        if len(train_pred.shape) > 1 and train_pred.shape[1] == 2:\n",
    "            train_pred = train_pred[:,1]\n",
    "            test_pred = test_pred[:,1]\n",
    "\n",
    "        train_ensemble.loc[test_indices,\"predictions_\" + model.__class__.__name__] = train_pred\n",
    "        test_ensemble[\"predictions\" + model.__class__.__name__] = test_pred\n",
    "        \n",
    "        df = pd.DataFrame({\"is_duplicate\": test_pred, \"Id\": test.index})\n",
    "        df.to_csv(\"preds_\" + model.__class__.__name__ + str(j) + \".csv\")\n",
    "        log_loss_fold.append(log_loss(y_test, train_pred))\n",
    "                \n",
    "    loss = np.mean(log_loss_fold)\n",
    "    results[model.__class__.__name__] = loss\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Result model {}: {}\".format(model.__class__.__name__,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_ensemble, train_all.have_same_meaning, test_size=0.2, random_state=4242)\n",
    "\n",
    "d_train = lgb.Dataset(X_train, label=y_train)\n",
    "d_valid = lgb.Dataset(X_valid, label=y_valid)\n",
    "d_test = lgb.Dataset(test_ensemble)\n",
    "\n",
    "# Set our parameters for xgboost\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 5,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.6,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0,\n",
    "    'max_depth':5\n",
    "}\n",
    "bst = lgb.train(params,\n",
    "                d_train,\n",
    "                num_boost_round=10000,\n",
    "                verbose_eval=300,\n",
    "                valid_sets=[d_train, d_valid],\n",
    "                early_stopping_rounds=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lgb.plot_importance(bst, max_num_features=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = bst.best_iteration\n",
    "\n",
    "d_train = lgb.Dataset(train_ensemble, label=train_all.have_same_meaning)\n",
    "d_test = lgb.Dataset(test_ensemble)\n",
    "\n",
    "bst = lgb.train(params,\n",
    "                d_train,\n",
    "                num_boost_round=n_iter,\n",
    "                valid_sets=[d_train],\n",
    "                verbose_eval=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Features for weighted graph\n",
    "\n",
    "# y_test_pred = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "# y_train_pred = bst.predict(train, num_iteration=bst.best_iteration)\n",
    "\n",
    "# df_test_res = pd.DataFrame(y_test_pred, columns=[\"weight\"])\n",
    "# df_test_res.to_csv(\"./data/predictions_test.csv\")\n",
    "\n",
    "# df_train_res = pd.DataFrame(y_train_pred, columns=[\"weight\"])\n",
    "# df_train_res.to_csv(\"./data/predictions_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bst.predict(test_ensemble, num_iteration=bst.best_iteration)\n",
    "submission = pd.DataFrame(test.index.values, columns=[\"Id\"])\n",
    "submission[\"Score\"] = y_pred\n",
    "submission.to_csv(\"submission.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
